{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import fairlearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.neighbors import DistanceMetric\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import pyclustering\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "import gower\n",
    "import seaborn as sns\n",
    "import pyclustering\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix as sklearn_confusion_matrix\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(X, cluster_instances):\n",
    "    cluster = X.loc[X.index == 0]\n",
    "    for value in cluster_instances:\n",
    "        tmp = X.loc[X.index == value]\n",
    "        cluster = pd.concat([cluster, tmp])\n",
    "    return cluster\n",
    "\n",
    "\n",
    "def confusion_matrix(X,Y, description, label):   \n",
    "    \"\"\"\n",
    "    Get all values for quality measure\n",
    "    ---------------------------------\n",
    "    n = # instances covered by the subgroup description\n",
    "    N = # instances of dataset\n",
    "    \n",
    "    tp = # instances covered by subgroup description and subgroup target\n",
    "    fp = # instances covered by subgroup description but not subgroup target\n",
    "    tn = # instances not covered by subgroup description and not subgroup target\n",
    "    fn = # instances not covered by subgroup description but subgroup target\n",
    "    \n",
    "    TP = # instances covered by the subgroup target\n",
    "    FP = # instances not covered by the subgroup target\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a mask to obtain all instances covered by description\n",
    "    mask = pd.Series([True] * len(X))\n",
    "    for feature, value in description.items():\n",
    "        mask &= (X[feature] == value)\n",
    "        \n",
    "    n = len(X[mask]) \n",
    "    N = len(X) \n",
    "    \n",
    "    tp = len(Y[mask].loc[Y[mask]['label'] == label])\n",
    "    fp = len(Y[mask].loc[Y[mask]['label'] == 1-label])\n",
    "    \n",
    "    TP = len(Y.loc[Y['label'] == label]) \n",
    "    FP = len(Y.loc[Y['label'] == 1-label])\n",
    "    \n",
    "    tn = N - n - fp\n",
    "    fn = TP - tp\n",
    "    \n",
    "\n",
    "    \n",
    "    return tp, fp, tn, fn, TP, FP\n",
    "\n",
    "# The following are from the VLSD article of Lopez-Martinez-Carrasco\n",
    "def sensitivity(tp, TP):\n",
    "    return tp/TP\n",
    "\n",
    "\n",
    "def specificity(fp, FP):\n",
    "    return (FP-fp)/FP\n",
    "\n",
    "def piatetsky_shapiro(tp, fp, TP, FP):\n",
    "    left = tp + fp\n",
    "    if left == 0:\n",
    "        return 0\n",
    "    \n",
    "    middle = tp/(tp+fp)\n",
    "    right = TP/(TP+FP)\n",
    "    return left * (middle-right)\n",
    "  \n",
    "\n",
    "def WRAcc(tp, fp, TP, FP):\n",
    "    if tp + fp == 0:\n",
    "        return 0\n",
    "    left = (tp+fp)/(TP+FP)\n",
    "    middle = tp/(tp+fp)\n",
    "    right = TP/(TP+FP)\n",
    "    return left * (middle-right)\n",
    "\n",
    "def WRAcc_optimistic(tp, fp, TP, FP):\n",
    "    if tp + fp == 0:\n",
    "        first = 0\n",
    "    else:\n",
    "        first = tp**2/(tp+fp)\n",
    "        \n",
    "    if TP + FP == 0:\n",
    "        second = 0\n",
    "    else:\n",
    "        second = TP/(TP+FP)\n",
    "        \n",
    "    return first*(1-second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: >=25.5 changed to 1\n",
      "Age: <25.5 changed to 0\n",
      "Credit amount: <3913.5 changed to 0\n",
      "Credit amount: >=3913.5 changed to 1\n",
      "Duration: <15.5 changed to 0\n",
      "Duration: >=15.5 changed to 1\n",
      "Job: 2 changed to 2\n",
      "Job: 1 changed to 3\n",
      "Job: 3 changed to 1\n",
      "Job: 0 changed to 4\n",
      "Sex: male changed to 1\n",
      "Sex: female changed to 0\n",
      "Housing: own changed to 1\n",
      "Housing: free changed to 2\n",
      "Housing: rent changed to 3\n",
      "Saving accounts: unknown changed to 4\n",
      "Saving accounts: little changed to 0\n",
      "Saving accounts: quite rich changed to 1\n",
      "Saving accounts: rich changed to 2\n",
      "Saving accounts: moderate changed to 3\n",
      "Checking account: little changed to 0\n",
      "Checking account: moderate changed to 1\n",
      "Checking account: unknown changed to 3\n",
      "Checking account: rich changed to 2\n",
      "Purpose: radio/TV changed to 5\n",
      "Purpose: education changed to 6\n",
      "Purpose: furniture/equipment changed to 3\n",
      "Purpose: car changed to 4\n",
      "Purpose: business changed to 1\n",
      "Purpose: domestic appliances changed to 7\n",
      "Purpose: repairs changed to 8\n",
      "Purpose: vacation/others changed to 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"../german_credit_data.xlsx\")\n",
    "df['label'] = (df.Risk == 'good').astype(int)\n",
    "df.drop(['Unnamed: 0','Risk'],axis=1,inplace=True)\n",
    "df = df.replace(np.nan, 'unknown', regex=True)\n",
    "df['Age'] = df['Age'].apply(lambda x: '>=25.5' if x >= 25.5 else '<25.5')\n",
    "df['Credit amount'] = df['Credit amount'].apply(lambda x: '>=3913.5' if x >= 3913.5 else '<3913.5')\n",
    "df['Duration'] = df['Duration'].apply(lambda x: '>=15.5' if x >= 15.5 else '<15.5')\n",
    "df = df.replace(np.nan, 'unknown', regex=True)\n",
    "changes = []\n",
    "changes_dict = {}\n",
    "data = df.loc[: , df.columns != 'label']\n",
    "labels = df.loc[: , df.columns == 'label']\n",
    "\n",
    "\n",
    "#Create lists of categorical and numerical features\n",
    "all_features = ['Age','Credit amount','Duration','Job','Sex', 'Housing','Saving accounts',\n",
    "                'Checking account','Purpose']\n",
    "num_features = []\n",
    "cat_features = all_features\n",
    "df.head()\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column in cat_features:\n",
    "    le.fit(data[column])\n",
    "    encoded_values = le.transform(data[column])\n",
    "    unique_labels = data[column].unique()\n",
    "    column_changes = {}\n",
    "    \n",
    "    for label, encoded_value in zip(unique_labels, encoded_values):\n",
    "        new_key = f\"{column}_{label}\"\n",
    "        # Check if the encoded value already exists for this column\n",
    "        while encoded_value in column_changes.values():\n",
    "            encoded_value += 1  # Increment until unique\n",
    "        changes.append(f\"{column}: {label} changed to {encoded_value}\")\n",
    "        column_changes[new_key] = encoded_value\n",
    "    \n",
    "    # Update the changes_dict with changes for the current column\n",
    "    changes_dict.update(column_changes)\n",
    "    data.loc[:, column] = encoded_values\n",
    "\n",
    "# print(\"List of changes:\")\n",
    "for change in changes:\n",
    "    print(change)\n",
    "    \n",
    "# 70% training and 30% test\n",
    "data = data.astype('int64')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, np.ravel(labels), test_size=0.30, random_state=78) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct predictions train set \t 75.14285714285714\n",
      "correct predictions test set \t 72.0\n",
      "correct predictions total set \t 74.2\n"
     ]
    }
   ],
   "source": [
    "# train logistic regression model and check performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, np.ravel(labels), test_size=0.30, random_state=78)\n",
    "clf = LogisticRegression(random_state=100,max_iter=10000).fit(X_train, y_train)\n",
    "clf_predictions_train = clf.predict(X_train)\n",
    "clf_predictions_test = clf.predict(X_test)\n",
    "clf_predictions_total = clf.predict(data)\n",
    "\n",
    "p_correct_train = 1 - (abs(clf_predictions_train-y_train).sum() / len(y_train))\n",
    "p_correct_test = 1 - (abs(clf_predictions_test-y_test).sum() / len(y_test))\n",
    "p_correct_total = 1 - (abs(clf_predictions_total-np.ravel(labels)).sum() / len(np.ravel(labels)))\n",
    "\n",
    "print('correct predictions train set' ,'\\t', p_correct_train*100)\n",
    "print('correct predictions test set' , '\\t',  p_correct_test*100)\n",
    "print('correct predictions total set' , '\\t', p_correct_total*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: array with predicted labels, array with true labels\n",
    "#output: percentage correct predicted labels \n",
    "\n",
    "def wrong_disadvantage(predicted_labels,true_labels):\n",
    "    count = 0 \n",
    "    size = len(true_labels)\n",
    "    for i in np.arange(size):\n",
    "        if ((predicted_labels[i]==0) & (true_labels[i]==1)):\n",
    "            count += 1\n",
    "    output = count / size\n",
    "    return output    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fairness based on description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truths(clf, data, label, description, sensitive_attribute='Sex'):\n",
    "    # Create a mask to obtain all instances covered by description\n",
    "    mask = pd.Series([True] * len(data))\n",
    "    for feature, value in description.items():\n",
    "        mask &= (data[feature] == value)\n",
    "    \n",
    "    data_labels = labels\n",
    "    data_predicted = clf.predict(data)\n",
    "    subgroup_labels = data_labels[mask]\n",
    "    subgroup_predicted = data_predicted[mask]\n",
    "    \n",
    "    \n",
    "    # something is fair when the tp and tn of the subgroup are the same as the tp and tn of the data overall?\n",
    "    subgroup_true_positives = np.sum((subgroup_predicted == 1) & (subgroup_labels['label'] == 1))\n",
    "    subgroup_true_negatives = np.sum((subgroup_predicted == 0) & (subgroup_labels['label'] == 0))\n",
    "    subgroup_false_positives = np.sum((subgroup_predicted == 1) & (subgroup_labels['label'] == 0))\n",
    "    subgroup_false_negatives = np.sum((subgroup_predicted == 0) & (subgroup_labels['label'] == 1))\n",
    "    \n",
    "    true_positives = np.sum((data_predicted == 1) & (data_labels['label'] == 1))\n",
    "    true_negatives = np.sum((data_predicted == 0) & (data_labels['label'] == 0))\n",
    "    false_positives = np.sum((data_predicted == 1) & (data_labels['label'] == 0))\n",
    "    false_negatives = np.sum((data_predicted == 0) & (data_labels['label'] == 1))\n",
    "\n",
    "    ground_truths_subgroup = [subgroup_true_positives, subgroup_true_negatives, subgroup_false_positives, \n",
    "                             subgroup_false_negatives]\n",
    "    ground_truths_data = [true_positives, true_negatives, false_positives, false_negatives]\n",
    "    \n",
    "    return ground_truths_subgroup, ground_truths_data\n",
    "\n",
    "\n",
    "def positives_subgroup(ground_truths_subgroup, ground_truths_data):\n",
    "    tp = ground_truths_data[0]\n",
    "    tn = ground_truths_data[1]\n",
    "    fp = ground_truths_data[2]\n",
    "    fn = ground_truths_data[3]\n",
    "    \n",
    "    tp_s = ground_truths_subgroup[0]\n",
    "    tn_s = ground_truths_subgroup[1]\n",
    "    fp_s = ground_truths_subgroup[2]\n",
    "    fn_s = ground_truths_subgroup[3]\n",
    "       \n",
    "    data_size = tp+fp+tn+fn\n",
    "    subgroup_size = tp_s+fp_s+tn_s+fn_s\n",
    "    P_D = (tp+fp)/data_size\n",
    "    P_D_g = (tp_s+fp_s)/subgroup_size\n",
    "    \n",
    "    #based on the assumption that each instance is evenly likely to belong to this subgroup\n",
    "    alpha_P = subgroup_size/data_size \n",
    "#     beta_SP = np.abs(SP_D - SP_D_g) # now we don't know if the bias is positive or negative\n",
    "    beta_P = P_D - P_D_g\n",
    "    \n",
    "    return alpha_P, P_D, P_D_g\n",
    "\n",
    "def demographic_parity_subgroup(P_D, P_D_g):\n",
    "    return np.abs(P_D-P_D_g)\n",
    "\n",
    "\n",
    "def false_positives_subgroup(ground_truths_subgroup, ground_truths_data):  \n",
    "    tp = ground_truths_data[0]\n",
    "    tn = ground_truths_data[1]\n",
    "    fp = ground_truths_data[2]\n",
    "    fn = ground_truths_data[3]\n",
    "    \n",
    "    tp_s = ground_truths_subgroup[0]\n",
    "    tn_s = ground_truths_subgroup[1]\n",
    "    fp_s = ground_truths_subgroup[2]\n",
    "    fn_s = ground_truths_subgroup[3]\n",
    "       \n",
    "    data_size = tp+fp+tn+fn\n",
    "    subgroup_size = tp_s+fp_s+tn_s+fn_s\n",
    "\n",
    "    FP_D = fp/(fp+tn)\n",
    "    if fp_s + tn_s == 0:\n",
    "        FP_D_g = 0\n",
    "    else:\n",
    "        FP_D_g = fp_s/(fp_s+tn_s)   \n",
    "    \n",
    "    alpha_FP = (fp_s+tn_s)/data_size\n",
    "#     beta_FP = np.abs(FP_D - FP_D_g) # now we don't know if the bias is positive or negative\n",
    "    beta_FP = FP_D - FP_D_g\n",
    "    return alpha_FP, beta_FP\n",
    "\n",
    "def true_positives_subgroup(ground_truths_subgroup, ground_truths_data):  \n",
    "    tp = ground_truths_data[0]\n",
    "    tn = ground_truths_data[1]\n",
    "    fp = ground_truths_data[2]\n",
    "    fn = ground_truths_data[3]\n",
    "    \n",
    "    tp_s = ground_truths_subgroup[0]\n",
    "    tn_s = ground_truths_subgroup[1]\n",
    "    fp_s = ground_truths_subgroup[2]\n",
    "    fn_s = ground_truths_subgroup[3]\n",
    "       \n",
    "    data_size = tp+fp+tn+fn\n",
    "    subgroup_size = tp_s+fp_s+tn_s+fn_s\n",
    "\n",
    "    TP_D = tp/(tp+fn)\n",
    "    if tp_s + fn_s == 0:\n",
    "        TP_D_g = 0\n",
    "    else:\n",
    "        TP_D_g = tp_s/(tp_s+fn_s)   \n",
    "    \n",
    "    alpha_TP = (tp_s+fn_s)/data_size\n",
    "#     beta_FP = np.abs(FP_D - FP_D_g) # now we don't know if the bias is positive or negative\n",
    "    beta_TP = TP_D - TP_D_g\n",
    "    return alpha_TP, beta_TP\n",
    "\n",
    "def equalized_odds_subgroup(TPD, FPD):\n",
    "    return max(np.abs(TPD), np.abs(FPD))\n",
    "#     if TPD < 0 and FPD < 0:\n",
    "#         return min(TPD, FPD)\n",
    "#     else:\n",
    "#         return max(np.abs(TPD), np.abs(FPD))\n",
    "    \n",
    "    \n",
    "\n",
    "# gt_sg, gt_d = get_ground_truths(clf, data, labels, {'Checking account': 3})   \n",
    "# statistical_parity_subgroup(gt_sg, gt_d)\n",
    "# false_positives_subgroup(gt_sg, gt_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_based_on_descriptions(data, label, description, sensitive_attribute='Sex'):\n",
    "    bias_clf = {\n",
    "        'Accuracy': [],\n",
    "        'Wrong disadvantage': [],\n",
    "        \n",
    "        'Recall': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': [],\n",
    "        'piatetsky_shapiro': [],\n",
    "        'WRAcc_score': [],\n",
    "        'WRAcc_score_optimistic': [],\n",
    "        \n",
    "        'demographic_parity_difference' : [],\n",
    "        'equalized_odds_difference' : [],\n",
    "        'demographic_parity_subgroup': [],\n",
    "        'equalized_odds_subgroup': [],\n",
    "        'true_positive_subgroup': [],\n",
    "        'false_positive_subgroup': [],\n",
    "        'weighted_dpd_subgroup': [],\n",
    "        'weighted_eod_subgroup': [],\n",
    "        'weighted_tp_subgroup': [],\n",
    "        'weighted_fp_subgroup': []\n",
    "    }\n",
    "       \n",
    "    tp, fp, tn, fn, TP, FP = confusion_matrix(data,label, description, label=1)\n",
    "\n",
    "    sensitivit = sensitivity(tp, TP)\n",
    "    specificit = specificity(fp, FP)\n",
    "    shapiro = piatetsky_shapiro(tp, fp, TP, FP)\n",
    "    WRAcc_score = WRAcc(tp ,fp, TP, FP)\n",
    "    WRAcc_score_optimistic = WRAcc_optimistic(tp, fp, TP, FP)\n",
    "    \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, np.ravel(labels), test_size=0.30, random_state=78)\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = label.iloc[train_index], label.iloc[test_index]\n",
    "        clf = LogisticRegression(random_state=100,max_iter=10000)\n",
    "        clf.fit(X_train, np.ravel(y_train))\n",
    "        # Create a mask to obtain all instances covered by description\n",
    "        mask = pd.Series([True] * len(data))\n",
    "        for feature, value in description.items():\n",
    "            mask &= (data[feature] == value)\n",
    "\n",
    "        subset_X = data[mask]\n",
    "        subset_y = label[mask]\n",
    "\n",
    "        if len(subset_X) > 0:\n",
    "            clf_predictions_description = clf.predict(subset_X)\n",
    "        else:\n",
    "            return pd.DataFrame(bias_clf)\n",
    "\n",
    "\n",
    "        acc = accuracy_score(np.ravel(subset_y), clf_predictions_description)\n",
    "\n",
    "        recall = recall_score(np.ravel(subset_y), clf_predictions_description, pos_label=1, zero_division=0.0)\n",
    "        score = wrong_disadvantage(clf_predictions_description,np.ravel(subset_y))\n",
    "\n",
    "        dp = demographic_parity_difference(subset_y, clf_predictions_description, \n",
    "                                           sensitive_features=subset_X[sensitive_attribute])\n",
    "        eo = equalized_odds_difference(subset_y, clf_predictions_description, \n",
    "                                       sensitive_features=subset_X[sensitive_attribute])\n",
    "\n",
    "\n",
    "        gt_sg, gt_d = get_ground_truths(clf, data, label, description) \n",
    "        alpha_P, P_D, P_D_g = positives_subgroup(gt_sg, gt_d)\n",
    "        beta_P = demographic_parity_subgroup(P_D, P_D_g)\n",
    "        alpha_FP, beta_FP = false_positives_subgroup(gt_sg, gt_d)\n",
    "        alpha_TP, beta_TP = true_positives_subgroup(gt_sg, gt_d)\n",
    "        eod = equalized_odds_subgroup(beta_TP, beta_FP)\n",
    "        weod = equalized_odds_subgroup(alpha_TP*beta_TP, alpha_FP*beta_FP)\n",
    "    \n",
    "        bias_clf['Accuracy'].append(acc)\n",
    "        bias_clf['Wrong disadvantage'].append(score)\n",
    "\n",
    "        bias_clf['Recall'].append(recall)\n",
    "        bias_clf['sensitivity'].append(sensitivit)\n",
    "        bias_clf['specificity'].append(specificit)\n",
    "        bias_clf['piatetsky_shapiro'].append(shapiro)\n",
    "        bias_clf['WRAcc_score'].append(WRAcc_score)\n",
    "        bias_clf['WRAcc_score_optimistic'].append(WRAcc_score_optimistic)\n",
    "\n",
    "        bias_clf['demographic_parity_difference'].append(dp)\n",
    "        bias_clf['equalized_odds_difference'].append(eo)\n",
    "        bias_clf['demographic_parity_subgroup'].append(beta_P)\n",
    "        bias_clf['equalized_odds_subgroup'].append(eod)\n",
    "        bias_clf['false_positive_subgroup'].append(beta_FP)\n",
    "        bias_clf['true_positive_subgroup'].append(beta_TP)\n",
    "        bias_clf['weighted_dpd_subgroup'].append(alpha_P*beta_P)\n",
    "        bias_clf['weighted_eod_subgroup'].append(weod)\n",
    "        bias_clf['weighted_tp_subgroup'].append(alpha_TP*beta_TP)\n",
    "        bias_clf['weighted_fp_subgroup'].append(alpha_FP*beta_FP)\n",
    "\n",
    "    \n",
    "    df_bias = pd.DataFrame(bias_clf)\n",
    "    df_bias = pd.DataFrame(df_bias.mean()).T\n",
    "    df_bias['description'] = [description]\n",
    "    return df_bias\n",
    "\n",
    "\n",
    "def allow_comparison_clustering(all_descriptions, data, label):\n",
    "    comparison_clustering = pd.DataFrame(columns=[\n",
    "        'description',\n",
    "        'Accuracy',\n",
    "        'Wrong disadvantage',\n",
    "        'Recall',\n",
    "        'sensitivity',\n",
    "        'specificity',\n",
    "        'piatetsky_shapiro',\n",
    "        'WRAcc_score',\n",
    "        'WRAcc_score_optimistic',\n",
    "        'demographic_parity_difference',\n",
    "        'equalized_odds_difference'])\n",
    "\n",
    "    for description in all_descriptions:\n",
    "        changed_description = {}\n",
    "        for key, value in description.items():\n",
    "            original_key = f\"{key}_{value}\"\n",
    "            if original_key in changes_dict:\n",
    "                changed_description[key] = changes_dict[original_key]\n",
    "       \n",
    "        row = fairness_based_on_descriptions(data, label, changed_description, sensitive_attribute='Sex').round(4)\n",
    "\n",
    "        comparison_clustering = pd.concat([comparison_clustering, row], ignore_index=False)\n",
    "        \n",
    "    return comparison_clustering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Subgroup Discovery vs Cluster discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_changes = [\n",
    "    {'Purpose': ['repairs', 8]}, \n",
    "    {'Purpose': ['vacation/others', 2]}, \n",
    "    {'Purpose': ['domestic appliances', 7]}, \n",
    "    {'Purpose': ['business', 1]}, \n",
    "    {'Purpose': ['car', 4]}, \n",
    "    {'Purpose': ['furniture/equipment', 3]}, \n",
    "    {'Purpose': ['education', 6]}, \n",
    "    {'Purpose': ['radio/TV', 5]}, \n",
    "    {'Checking account': ['rich', 2]}, \n",
    "    {'Checking account': ['unknown', 3]}, \n",
    "    {'Checking account': ['moderate', 1]}, \n",
    "    {'Checking account': ['little', 0]}, \n",
    "    {'Saving accounts': ['moderate', 3]}, \n",
    "    {'Saving accounts': ['rich', 2]}, \n",
    "    {'Saving accounts': ['quite rich', 1]}, \n",
    "    {'Saving accounts': ['little', 0]}, \n",
    "    {'Saving accounts': ['unknown', 4]}, \n",
    "    {'Housing': ['rent', 3]}, \n",
    "    {'Housing': ['free', 2]}, \n",
    "    {'Housing': ['own', 1]}, \n",
    "    {'Job': [0, 4]}, \n",
    "    {'Job': [3, 1]}, \n",
    "    {'Job': [1, 3]}, \n",
    "    {'Job': [2, 2]}, \n",
    "    {'Sex': ['female', 0]}, \n",
    "    {'Sex': ['male', 1]},\n",
    "    {'Age': ['>=25.5', 1]},\n",
    "    {'Age': ['<25.5', 0]},\n",
    "    {'Duration': ['<15.5', 0]},\n",
    "    {'Duration': ['>=15.5', 1]},\n",
    "    {'Credit amount': [ '<3913.5', 0]},\n",
    "    {'Credit amount': [ '>=3913.5', 1]}\n",
    "]\n",
    "   \n",
    "def find_feature_change(list_of_changes, feature, value):\n",
    "    for change in list_of_changes:\n",
    "        if feature in change:\n",
    "            new_value, old_value = change[feature]\n",
    "            if old_value == value:\n",
    "                return new_value\n",
    "    return value\n",
    "\n",
    "def translate_format_comparison(comparison_clustering):\n",
    "    list_of_descriptions = list(comparison_clustering['description'])\n",
    "    list_of_new_descriptions = []\n",
    "    \n",
    "    for description in list_of_descriptions:\n",
    "        new_description = {}\n",
    "        for feature, value in description.items():\n",
    "            new_value = find_feature_change(list_of_changes, feature, value)\n",
    "            new_description[feature] = new_value\n",
    "        list_of_new_descriptions.append(new_description)\n",
    "\n",
    "    comparison_clustering['description'] = list_of_new_descriptions\n",
    "    \n",
    "    return comparison_clustering\n",
    "\n",
    "def preprocess_string_to_dict(s):\n",
    "    try:\n",
    "        return json.loads(s.replace(\"'\", '\"'))\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "    \n",
    "\n",
    "def preprocess_descriptions(df):\n",
    "    df['description'] = df['description'].apply(preprocess_string_to_dict)\n",
    "    return list(df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSD_results = pd.read_csv('../PSD/PSD_goodcredit_results.csv')[:100]    \n",
    "PSD_results.to_csv('../PSD/PSD_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_descriptions = pd.read_csv('../DFS/DFS_descriptions.csv')\n",
    "dfs_descriptions = preprocess_descriptions(dfs_descriptions)\n",
    "dfs_results = allow_comparison_clustering(dfs_descriptions, data, labels)\n",
    "dfs_results = translate_format_comparison(dfs_results)\n",
    "dfs_results = dfs_results.sort_values(by='WRAcc_score', ascending=False)\n",
    "dfs_results.to_csv('../DFS/DFS_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlsd_descriptions = pd.read_csv('../VLSD/VLSD_descriptions.csv')[:100]\n",
    "vlsd_descriptions = preprocess_descriptions(vlsd_descriptions)\n",
    "vlsd_results = allow_comparison_clustering(vlsd_descriptions, data, labels)\n",
    "vlsd_results = translate_format_comparison(vlsd_results)\n",
    "vlsd_results = vlsd_results.sort_values(by='WRAcc_score', ascending=False)\n",
    "vlsd_results.to_csv('../VLSD/VLSD_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
